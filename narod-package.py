# üíø –ò–ú–ü–û–†–¢ –ù–ï–û–ë–•–û–î–ò–ú–´–• –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô

import requests
from bs4 import BeautifulSoup as Bs  # —Å—É–ø –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ html —Ä–∞–∑–º–µ—Ç–∫–∏
from urllib.parse import urlparse, urljoin, urlsplit
import concurrent.futures
import ast
import random
import time
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

USER_AGENTS = [
    # –°–ø–∏—Å–æ–∫ user-agent —Å—Ç—Ä–æ–∫ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0"

]


# ‚öôÔ∏è –°–ü–ò–°–û–ö –ò–°–ü–û–õ–¨–ó–£–ï–ú–´–• –§–£–ù–ö–¶–ò–ô

# –§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–º–µ–Ω–∞
def extract_url(domain):
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞.

    Args:
        domain (str): –î–æ–º–µ–Ω–Ω–æ–µ –∏–º—è –±–µ–∑ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∏ –ø—É—Ç–∏.

    Returns:
        str: –ü–æ–ª–Ω—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–¥—Ä–µ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    url = f'https://{domain}.narod.ru'

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    return url


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏
def random_delay():
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –æ—Ç 1 –¥–æ 3 —Å–µ–∫—É–Ω–¥."""
    time.sleep(random.uniform(1, 3))


def create_session():
    """–°–æ–∑–¥–∞–µ—Ç —Å–µ—Å—Å–∏—é —Å retries –∏ cookies."""
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=0.1,
                    status_forcelist=[500, 502, 503, 504])
    session.mount('https://', HTTPAdapter(max_retries=retries))
    session.headers.update({"User-Agent": random.choice(USER_AGENTS)})
    return session


def get_internal_links(url):
    """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ URL.

    Args:
        url (str): URL —Å–∞–π—Ç–∞ –¥–ª—è —Å–±–æ—Ä–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫.

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å URL –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å–∞–º–∏.
    """
    urls = dict()
    origin_host = urlparse(url).netloc
    get_page_links(url, urls, origin_host)
    del urls[urlsplit(url)._replace(path='/').geturl()]
    return urls


def get_page_links(url, urls, origin_host):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.

    Args:
        url (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
        urls (dict): –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è URL –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å–æ–≤.
        origin_host (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ö–æ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫.
    """
    if url in urls:
        return

    try:
        session = create_session()
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        response = session.get(url, headers=headers, timeout=5)
        random_delay()

        if 'Content-Type' not in response.headers or not response.headers['Content-Type'].startswith('text/html'):
            return

        status = response.status_code
        if status != 200:
            urls[url] = status
            return

        urls[url] = status
        print(f'Parsing URL: {url}')

        html = response.content
        soup = Bs(html, 'html.parser')

        links = set()
        for a in soup.find_all(('a', 'area'), {'href': True}):
            href = a.attrs['href']

            if href.startswith(('javascript:', 'mailto:')) or '#' in href:
                continue

            if urlparse(href).netloc in (origin_host, ''):
                href_link = urljoin(url, href, allow_fragments=False)
                if urlparse(href_link).scheme != 'https':
                    href_link = urlsplit(href_link)._replace(
                        scheme='https').geturl()

                if not is_valid_link(href_link):
                    continue
                else:
                    links.add(href_link)

        for link in links:
            get_page_links(link, urls, origin_host)

    except requests.RequestException as e:
        print(f"Request error for {url}: {e}")
        urls[url] = 'Request error'

    except Exception as e:
        print(f"General error for {url}: {e}")
        urls[url] = 'General error'


def is_valid_link(href_link):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å—Å—ã–ª–∫–∏.

    Args:
        href_link (str): –°—Å—ã–ª–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.

    Returns:
        bool: True, –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –≤–∞–ª–∏–¥–Ω–∞, –∏–Ω–∞—á–µ False.
    """
    return not (urlparse(href_link).path in ('/index.html', '/register', '', '/') or
                urlparse(href_link).path.startswith(('/gb', '/panel')))


def scrape_site(url):
    """–ü–æ–ª—É—á–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞.

    Args:
        url (str): URL —Å–∞–π—Ç–∞.

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å–∞–º–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        return get_internal_links(url)
    except Exception as e:
        print(f"Error scraping site {url}: {e}")
        return None


def scrape_multiple_sites(input_df, url_column):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∞–π—Ç–æ–≤ –∏–∑ DataFrame.

    Args:
        input_df (pd.DataFrame): DataFrame —Å URL —Å–∞–π—Ç–æ–≤.
        url_column (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å URL.

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å–∞–º–∏.
    """
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_url = {executor.submit(
            scrape_site, url): url for url in input_df[url_column]}
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"Error processing {url}: {e}")
                results.append(None)
    return results


def extract_domain_from_link(internal_links_dict):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ —Å–ª–æ–≤–∞—Ä—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫.

    Args:
        internal_links_dict (dict): –°–ª–æ–≤–∞—Ä—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫.

    Returns:
        str: –î–æ–º–µ–Ω–Ω–æ–µ –∏–º—è.
    """
    if internal_links_dict:
        domain = urlparse(list(internal_links_dict.keys())[0]).netloc[:-9]
        return domain


# –§—É–Ω–∫—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–∞ html –∏–∑ —Ñ—Ä–µ–π–º–æ–≤
def extract_frames_from_html(link, html_pages):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ—Ä–µ–π–º–æ–≤ (frame –∏ iframe) –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏—Ö –≤ html_pages.

    Args:
        link (str): URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        html_pages (dict): –°–ª–æ–≤–∞—Ä—å, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π HTML –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Returns:
        dict: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å HTML —Å–æ–¥–µ—Ä–∂–∏–º—ã–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –µ–µ —Ñ—Ä–µ–π–º–æ–≤.
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º soup-–æ–±—ä–µ–∫—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML-–¥–µ—Ä–µ–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–π—Ç–∞
    try:
        for path in list(html_pages.keys()):
            soup = Bs(html_pages[path], 'html.parser')

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ frame –∏ iframe —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º src
            frames_list = soup.find_all(['frame', 'iframe'], {'src': True})
            for frame in frames_list:
                frame_source = frame.attrs['src']
                frame_url = urljoin(link, frame_source)
                response = requests.get(frame_url)

                if response.status_code == 200:
                    frame_html = response.content.decode(
                        'utf-8', errors='ignore')
                    html_pages[f'{path}_{frame.name}_{
                        frame_source}'] = frame_html
                else:
                    html_pages[f'{path}_{frame.name}_{
                        frame_source}'] = response.status_code

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ iframe —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º srcdoc
            iframes_list = soup.find_all('iframe', {'srcdoc': True})
            for iframe in iframes_list:
                frame_html = iframe.attrs['srcdoc']
                html_pages[f'{path}_{iframe.name}_srcdoc'] = frame_html

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å HTML-—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Ñ—Ä–µ–π–º–æ–≤
        return html_pages

    except requests.RequestException as e:
        print(f"Request error for frame link:{link}: {e}")

    except Exception as e:
        print(f"General error for frame {link}: {e}")


def extract_html(links_dict):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –µ–µ —Ñ—Ä–µ–π–º–æ–≤ (–µ—Å–ª–∏ –∏–º–µ—é—Ç—Å—è).

    Args:
        links_dict (dict): –°–ª–æ–≤–∞—Ä—å —Å—Å—ã–ª–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    Returns:
        dict or int: –°–ª–æ–≤–∞—Ä—å —Å HTML —Å–æ–¥–µ—Ä–∂–∏–º—ã–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Ñ—Ä–µ–π–º–æ–≤, –∏–ª–∏ –∫–æ–¥ –æ—à–∏–±–∫–∏ HTTP.
    """
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –∏ –∑–∞–¥–∞–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∑–∞–ø—Ä–æ—Å–∞ User-Agent –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏ –ø–æ–¥ –±—Ä–∞—É–∑–µ—Ä
    try:
        session = create_session()
        headers = {"User-Agent": random.choice(USER_AGENTS)}

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è HTML-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º –¥–æ–º–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –º–µ—Ä–¥–∂–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ü–µ–ª–µ–≤—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
        html_pages = {'domain': extract_domain_from_link(links_dict)}

        # –î–µ–ª–∞–µ–º GET-–∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        for link, status in list(links_dict.items()):
            if status == 200:
                response = session.get(
                    link, headers=headers, allow_redirects=False)
                random_delay()

                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ UTF-8
                html = response.content.decode('utf-8', errors='ignore')
                link_path = urlparse(link).path
                html_pages[link_path] = html

        return extract_frames_from_html(link, html_pages)

    except requests.RequestException as e:
        print(f"Request error for {link}: {e}")

    except Exception as e:
        print(f"General error for {link}: {e}")


def delete_ucoz(html_dict):
    """
    –û—á–∏—â–∞–µ—Ç HTML-—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç —Ä–µ–∫–ª–∞–º—ã Ucoz.

    Args:
        html (str): –°—Ç—Ä–æ–∫–∞ —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π —Å–ª–æ–≤–∞—Ä—è htmlPages, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–±–æ –≤—Å–µ—Ö HTML-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    Returns:
        dict or str: –°–ª–æ–≤–∞—Ä—å —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ HTML-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ HTTP-–∫–æ–¥–∞ –æ—à–∏–±–∫–∏.
    """
    # –ï—Å–ª–∏ –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏ > 3, —Ç.–µ. –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è HTTP-–∫–æ–¥–æ–º –æ—à–∏–±–∫–∏, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 3 —Å–∏–º–≤–æ–ª–æ–≤
    if len(html_dict) > 3:
        # –û–±—ä—è–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é - –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å, –≤ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥—É—Ç —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å—Å—è –æ—á–∏—â–µ–Ω–Ω—ã–µ –æ—Ç —Ä–µ–∫–ª–∞–º—ã Ucoz HTML-—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        clean_dict = {}

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π —Å–ª–æ–≤–∞—Ä—è htmlPages –≤ —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ast
        # html_to_dict = ast.literal_eval(html)

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞ –∏ –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä–µ
        for k, v in html_dict.items():
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å clean_dict —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ñ—É–Ω–∫—Ü–∏–∏ delete_ucoz_html, –∫–æ—Ç–æ—Ä–∞—è –æ—á–∏—â–∞–µ—Ç –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–π HTML-–¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç —Ä–µ–∫–ª–∞–º—ã Ucoz,
            # –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –µ–º—É –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–π –∫–ª—é—á
            clean_dict[k] = delete_ucoz_html(v)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å HTML-—Å–æ–¥–µ—Ä–∂–∏–º—ã–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –µ–µ —Ñ—Ä–µ–π–º–∞–º–∏ (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏), –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –æ—Ç —Ä–µ–∫–ª–∞–º—ã Ucoz
        return clean_dict
    else:
        # –ò–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Ö–æ–¥–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        return html_dict


def delete_ucoz_html(html):
    """
    –û—á–∏—â–∞–µ—Ç HTML –æ—Ç —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –±–ª–æ–∫–æ–≤ Ucoz.

    Args:
        html (str or int): HTML-–¥–æ–∫—É–º–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã/—Ñ—Ä–µ–π–º–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –∫–æ–¥ HTTP –æ—à–∏–±–∫–∏.

    Returns:
        str or int: –û—á–∏—â–µ–Ω–Ω—ã–π HTML-–¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ HTTP –æ—à–∏–±–∫–∏.
    """
    # –ï—Å–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è html - —á–∏—Å–ª–æ, —Ç.–µ. —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–¥ HTTP –æ—à–∏–±–∫–∏, –≤–æ–∑–Ω–∏–∫—à–µ–π –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    if isinstance(html, int):
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—É –≤—Ö–æ–¥–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é html
        return html
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML, —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å —Ä–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏
        soup = Bs(html, "html.parser")

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ç—ç–≥–∏ <script> –≤ —Ç—ç–≥–µ <head>
        try:
            head_scripts = soup.head.find_all('script')
            if head_scripts:
                [tag.decompose() for tag in head_scripts]
        except AttributeError:
            # –ï—Å–ª–∏ —É–∂–µ –±—ã–ª–∞ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∞ –æ—á–∏—Å—Ç–∫–∞, —Ç–æ –ø—Ä–æ—Å—Ç–æ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏—è –≤—ã—à–µ
            pass

        # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫—É, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —ç–ª–µ–º–µ–Ω—Ç–µ Copyright (—Ä–∞—Å–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –≤–Ω–∏–∑—É —Ä–∞–∑–º–µ—Ç–∫–∏)
        ucoz_copyright = soup.find("a", {"href": "https://www.ucoz.ru/"})

        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ copyright
        if ucoz_copyright:
            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫—É
            ucoz_copyright.parent.decompose()

        # –û—á–∏—â–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç BeautifulSoup –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Å—Ç—Ä–æ–∫—É, —É–¥–∞–ª—è—è –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞
        # (whitespace –æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —É–¥–∞–ª–µ–Ω–∏—è —Ä–µ–∫–ª–∞–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫)
        no_ucoz_page = str(soup).strip()

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return no_ucoz_page


def extract_text(html):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ HTML-–¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Args:
        html (dict or str): –ó–Ω–∞—á–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ html –∏–∑ DataFrame, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º –∏–ª–∏ —Å—Ç—Ä–æ–∫–æ–π.

    Returns:
        dict or str: –°–ª–æ–≤–∞—Ä—å —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º HTML-–¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –µ—Å–ª–∏ –≤—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º.
    """
    # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–º–µ–µ—Ç —Ç–∏–ø dict (—Å–ª–æ–≤–∞—Ä—å)
    if isinstance(html, dict):
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–∞–∂–¥–æ–π HTML-—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        text_dict = {}

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞ –∏ –∑–Ω–∞—á–µ–Ω–∏—è (HTML-–¥–æ–∫—É–º–µ–Ω—Ç–∞) –≤ —Å–ª–æ–≤–∞—Ä–µ html
        for k, v in html.items():
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –ø—Ä–æ–±–µ–ª–æ–≤ —Å –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏
            # –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º HTML-–¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Å–ª–æ–≤–∞—Ä—å text_dict
            text_dict[k + '_text'] = Bs(v, 'html.parser').get_text().strip()

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ HTML-–¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –µ–µ —Ñ—Ä–µ–π–º–æ–≤ (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏)
        return text_dict
    else:
        # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Ö–æ–¥–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        return html


def scrape_site_html(url):
    """–ü–æ–ª—É—á–∞–µ—Ç HTML-–¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞.

    Args:
        url (str): URL —Å–∞–π—Ç–∞.

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å html-–¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å–∞–º–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        return extract_html(url)
    except Exception as e:
        print(f"Error scraping site html {url}: {e}")
        return None


def scrape_multiple_sites_html(input_df, url_column):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä html-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–∞–π—Ç–æ–≤ –∏–∑ DataFrame.

    Args:
        input_df (pd.DataFrame): DataFrame —Å URL —Å–∞–π—Ç–æ–≤.
        url_column (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å URL.

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å html-–¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å–∞–º–∏.
    """
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_url = {executor.submit(
            scrape_site_html, url): url for url in input_df[url_column]}
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"Error processing {url}: {e}")
                results.append(None)
    return results


def delete_domain(input_dict):
    """–£–¥–∞–ª—è–µ—Ç –¥–æ–º–µ–Ω –∏–∑ –≤—Ö–æ–¥—è—â–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è 

    Args:
        input_dict (dict): –í—Ö–æ–¥—è—â–∏–π —Å–ª–æ–≤–∞—Ä—å

    Returns:
        input_dict (dict): –°–ª–æ–≤–∞—Ä—å —Å —É–¥–∞–ª–µ–Ω–Ω—ã–º –æ–±—ä–µ–∫—Ç–æ–º, –∏–º–µ—é—â–µ–º –∫–ª—é—á 'domain'
    """
    if isinstance(input_dict, dict):
        input_dict.__delitem__('domain')
    return input_dict


def convert_string_to_dict(string_dict):
    if isinstance(string_dict, str):
        string_dict = ast.literal_eval(string_dict)
    return string_dict


def extract_domain_from_dict(input_dict):
    return input_dict['domain']


def main():
    urls = get_internal_links('https://missterr.narod.ru/')
    for k, v in urls.items():
        print(k, v)


if __name__ == "__main__":
    main()
